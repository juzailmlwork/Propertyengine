{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from utils import create_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url=\"https://propertyengine.co.uk/properties?_0:(area:%27!!UUvK78GuGmouNr%27,filters:!((options:(hideNewBuild:!f,hideOwnershipScheme:!t,hideRetirement:!t,hideSoldSTCRemoved:!t),type:dontShow),(options:(range:!(110000,490000)),type:askingPrice),(options:(selectedTypes:!(HOUSE)),type:propertyType),(options:(range:!(2,4)),type:bedrooms),(options:(filter:!f),type:auction),(options:(range:!(%272023-02-25%27,now)),type:addedDate)),sort:newest)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "browser=create_driver()\n",
    "all_urls=[\"https://web.facebook.com/?_rdc=1&_rdr\",\"https://www.youtube.com/\"]\n",
    "for url in all_urls:\n",
    "    browser.execute_script('window.open(\"{}\", \"_blank\");'.format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser=create_driver()\n",
    "# Navigate to a website in the existing Chrome instance\n",
    "#main_url=\"https://propertyengine.co.uk/properties?_0:(area:ilford-greater-london,filters:!((options:(hideNewBuild:!f,hideOwnershipScheme:!t,hideRetirement:!t,hideSoldSTCRemoved:!t),type:dontShow),(options:(range:!(40000,!n)),type:askingPrice),(options:(range:!(3,4)),type:bedrooms)),sort:lowestPrice)\"\n",
    "#number_scrolls=30\n",
    "browser.get(main_url)\n",
    "wait = WebDriverWait(browser, 10)\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"*\")))\n",
    "time.sleep(10)\n",
    "html_content = browser.page_source\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "count = int(soup.find(\"div\" ,{\"class\": \"sc-hBxehG fyjDiA\"}).find_all(\"span\")[0].text.split(\" \")[2].replace(\",\",\"\"))\n",
    "print(count)\n",
    "#zoom the page\n",
    "#browser.execute_script(\"document.body.style.zoom='75%'\")#150\n",
    "\n",
    "recent_list = browser.find_element(By.XPATH, \"//div[@class='sc-6jdl74-3 gjtvLs']\")\n",
    "number_scrolls=int(count/2)+1\n",
    "# Scroll down 10 times within the div element\n",
    "all_urls=[]\n",
    "for i in tqdm(range(number_scrolls)):\n",
    "    if i!=0:\n",
    "        browser.execute_script(\"arguments[0].scrollTop += arguments[0].offsetHeight;\", recent_list)\n",
    "        time.sleep(5)#5\n",
    "    html_content = browser.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    mydivs = soup.find_all(\"a\" ,{\"class\": \"f1gc2u-0 zpUJr\"})\n",
    "\n",
    "    for element in mydivs:\n",
    "        #print(element[\"href\"])\n",
    "        #print(element)\n",
    "        all_urls.append(element[\"href\"])\n",
    "    all_urls=list(set(all_urls))\n",
    "    print(len(all_urls))\n",
    "    if len(all_urls)>count:\n",
    "        break\n",
    "browser.quit()\n",
    "\n",
    "#all_urls=list(set(all_urls))\n",
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tab(driver, url):\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    driver.get(f'https://propertyengine.co.uk{url}')\n",
    "    time.sleep(2)\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    mydivs1 = soup.find_all(\"div\" ,{\"class\": \"q3lrp5-3 cLGsWF\"})\n",
    "    asking_price_text=mydivs1[0].text[:]\n",
    "    mydivs3 = soup.find_all(\"div\" ,{\"class\": \"sc-gikAfH cnigbh\"})\n",
    "    print(url)\n",
    "    if len(mydivs3)!=0 and len(mydivs1)!=0:\n",
    "        #try:\n",
    "\n",
    "        if asking_price_text.count(\"£\")==2:\n",
    "            asking_price_text=asking_price_text.split(\"£\")[2]\n",
    "        else:\n",
    "            asking_price_text=asking_price_text[1:]\n",
    "\n",
    "        asking_price=int(asking_price_text.replace(',', ''))\n",
    "        average_price=int(mydivs3[0].text[1:].replace(',', ''))\n",
    "        print(asking_price,average_price)\n",
    "\n",
    "\n",
    "def scrape_urls(urls):\n",
    "    driver = create_driver()\n",
    "    for url in urls: \n",
    "        driver.execute_script(\"window.open('https://propertyengine.co.uk\" + url + \"');\")\n",
    "        time.sleep(0.5)\n",
    "    time.sleep(20)\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    for handle in driver.window_handles[:-1]:\n",
    "        time.sleep(3)\n",
    "        driver.switch_to.window(handle)\n",
    "        driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    for url in urls:\n",
    "        process_tab(driver, url)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls=pd.read_csv(\"all_urls.csv\")\n",
    "all_urls=list(all_urls[\"0\"])\n",
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tab(browser):\n",
    "    html_content = browser.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    mydivs1 = soup.find_all(\"div\" ,{\"class\": \"q3lrp5-3 cLGsWF\"})\n",
    "    asking_price_text=mydivs1[0].text[:]\n",
    "    mydivs3 = soup.find_all(\"div\" ,{\"class\": \"sc-gikAfH cnigbh\"})\n",
    "    if len(mydivs3)!=0 and len(mydivs1)!=0:\n",
    "        #try:\n",
    "\n",
    "        if asking_price_text.count(\"£\")==2:\n",
    "            asking_price_text=asking_price_text.split(\"£\")[2]\n",
    "        else:\n",
    "            asking_price_text=asking_price_text[1:]\n",
    "\n",
    "        asking_price=int(asking_price_text.replace(',', ''))\n",
    "        average_price=int(mydivs3[0].text[1:].replace(',', ''))\n",
    "        #print(asking_price,average_price)\n",
    "        return [asking_price,average_price]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def scrape_urls(urls):\n",
    "    url_info=[]\n",
    "    url_chunks = [urls[i:i+50] for i in range(0, len(urls), 50)]\n",
    "    print(len(url_chunks))\n",
    "    count=0\n",
    "    for chunk in url_chunks:\n",
    "        print(count)\n",
    "        driver = create_driver()\n",
    "        info={\"handles\":[],\"urls\":[]}\n",
    "        #handles = []\n",
    "        for url in chunk:\n",
    "            url_path=f'https://propertyengine.co.uk{url}'\n",
    "            driver.execute_script(\"window.open('{}');\".format(url_path))\n",
    "            info[\"handles\"].append(driver.window_handles[-1])\n",
    "            #driver.switch_to.window(info[\"handles\"][-1])\n",
    "            #url_path=f'https://propertyengine.co.uk{url}'\n",
    "            driver.get(url_path)\n",
    "            info[\"urls\"].append(url_path)\n",
    "        all_handles=info[\"handles\"]\n",
    "        all_urls=info[\"urls\"]\n",
    "        for index in range(len(all_handles)):\n",
    "        \n",
    "            driver.switch_to.window(all_handles[index])\n",
    "            try:\n",
    "                results=process_tab(driver)\n",
    "                if results==False:\n",
    "                    results=[0,0]\n",
    "                bye={\"url\":all_urls[index],\"asking_price\":results[0],\"average_price\":results[1]}\n",
    "                if results[1]-results[0]>75000:\n",
    "                    print(all_urls[index])\n",
    "                    print(results[0],results[1])\n",
    "                url_info.append(bye)\n",
    "            except:\n",
    "                pass\n",
    "            driver.close()\n",
    "        driver.quit()\n",
    "        time.sleep(10)\n",
    "        count+=1\n",
    "    return url_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_results=scrape_urls(all_urls[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results.extend(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser=create_driver()\n",
    "needed_url=[]\n",
    "for link in tqdm(all_urls[:18]):\n",
    "    url='https://propertyengine.co.uk{}'.format(link)\n",
    "    browser.get('https://propertyengine.co.uk{}'.format(link))\n",
    "    browser.execute_script(\"window.scrollBy(0,500)\")\n",
    "    #browser.execute_script(\"document.body.style.zoom='150%'\")\n",
    "    time.sleep(5)\n",
    "    html_content = browser.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    mydivs1 = soup.find_all(\"div\" ,{\"class\": \"q3lrp5-3 cLGsWF\"})\n",
    "    asking_price_text=mydivs1[0].text[:]\n",
    "    mydivs3 = soup.find_all(\"div\" ,{\"class\": \"sc-gikAfH cnigbh\"})\n",
    "    print(url)\n",
    "    if len(mydivs3)!=0 and len(mydivs1)!=0:\n",
    "        #try:\n",
    "\n",
    "        if asking_price_text.count(\"£\")==2:\n",
    "            asking_price_text=asking_price_text.split(\"£\")[2]\n",
    "        else:\n",
    "            asking_price_text=asking_price_text[1:]\n",
    "\n",
    "        asking_price=int(asking_price_text.replace(',', ''))\n",
    "        average_price=int(mydivs3[0].text[1:].replace(',', ''))\n",
    "        #print(url)\n",
    "        print(asking_price,average_price)\n",
    "        if average_price-asking_price>50000:#\n",
    "            needed_url.append(url)\n",
    "        # except:\n",
    "        #     pass\n",
    "    # Quit the browser\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_results))\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(main_results)\n",
    "#df.to_csv(\"csv_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.loc[df[\"average_price\"]-df[\"asking_price\"]>75000]\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"first_500_refined.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5202640946d9a272fdf5a6d16b99b38962d15028aa1bc782555374ed9a65f6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
